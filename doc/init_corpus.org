#+TITLE: Niconico大百科データセットを解析する
* Overview
  1. 圧縮ファイルをダウンロードする
  2. 解凍して配置する
  3. すべての記事ファイルについて前処理を施す
  4. 記事ヘッダについてデータベースにする
  5. データベースと組み合わせる + 前処理を施す
  6. 圧縮して保存する
* 圧縮ファイルをダウンロードする
  IDR の Niconico大百科データセット について利用申請をし、ダウンロードしてください。
* 解凍して配置する
  任意のディレクトリ下について、次のように解凍して配置してください。
#+begin_example
.
└── nico-dict
    └── zips
        ├── download.txt
        ├── head
        │   ├── head2008.csv
        │   ├── ...
        │   └── head2014.csv
        ├── head.zip
        ├── res
        │   ├── res2008.csv
        │   ├── ...
        │   └── res2014.csv
        ├── res.zip
        ├── rev2008.zip
        ├── rev2009
        │   ├── rev200901.csv
        │   ├── rev200902.csv
        │   ├── rev200903.csv
        │   ├── ...
        │   └── rev200912.csv
        ├── rev2009.zip
        ├──...
        ├── rev2013.zip
        ├── rev2014
        │   ├── rev201401.csv
        │   └── rev201402.csv
        └── rev2014.zip
#+end_example
* すべての記事ファイルについて前処理を施す
  Sehll ファイル [[../resources/preprocess.sh]] を ~zips~ へ配置して、実行して下さい。実行には15分位かかります。
  #+begin_src sh
    sh preprocess.sh
  #+end_src
** 前処理を施す理由
   https://www.tools.ietf.org/html/rfc4180#section-2 にあるように、二重引用符（”)は文中で用いられる際には重ねて書かれることが一般的です。
   しかし、このデータセットでは二重引用符をエスケープしています。つまり
   
   #+begin_example
   "<a href=""https://www.hogehoge.com/a.png"">"
   #+end_example

   とすべき部分を、
      #+begin_example
   "<a href=\"https://www.hogehoge.com/a.png\">"
   #+end_example
   
   としています。

   このため通常の（形式を踏襲している）解析ツールでこのデータを解析することが難しくなっています。
   先述のスクリプトはこれを解決するためのもので、一行目で 「\\"」 を 「""」 へ置換して、二行目で 「\\""」 を 「\\"」 としています。
   
   二行目については一行目で 「\\\\"」 というケースを巻き込んで変換してしまうものを修正するために必須のスクリプトとなっています。
** CSV の形式について
   記事本文のカラムは次のようになっています。
   #+begin_example
   [<記事ID>, <記事本文>, <記事更新日時>]
   #+end_example
     記事更新日時は YYYYMMDDhhmmss と推測されます。

* 記事ヘッダについてデータベースを構築する
  SQLファイル [[../resources/create-table.sql]] Shell ファイル [[../resources/import-dir.sh][../resources/import-dir.sh]]  を ~zips/head/~ へ配置して、実行してください。
  #+begin_src sh
  sh import-dir.sh
  #+end_src

  すると、headers.db というSQLが作成されると思います。このテーブルに記事ヘッダの情報が SQLite に保存されます。(ちなみに前章の前処理を施していないと，ここで事故を起こします。)

  データの例を見てみると、次のようになります。

  #+begin_example
  sqlite3 headers.db
  sqlite3 > select * from article_header limit 10
   ...> ;
   1|ニコニコ大百科|ニコニコダイヒャッカ|a|20080512173939
   4|カレー|カレー|a|20080512182423
   5|初音ミクにオリジナルソング「貴方に花を 私に唄を」を歌わせてみた。|\N|v|20080719234213
   9|ゴーゴーカレー|ゴーゴーカレー|a|20080512183606
   13|本格的 ガチムチパンツレスリング|\N|v|20080513225239
   27|頭がパーン(P)┗(^o^ )┓三|\N|v|20080529215132
   33|【初音ミク】『少し楽しくなる時報』【アレンジ曲】|\N|v|20080810020937
   37|【 SYNC.ART'S × U.N.オーエンは彼女なのか？ 】 −Sweets Time−|\N|v|20080616003242
   46|ニコニコ動画流星群|\N|v|20080513210124
   47|ハイポーション作ってみた。|\N|v|20090102150209
  #+end_example
  
* データベースと組み合わせる + 前処理を施す
  次のコマンドで実行できます。 Leiningen の実行環境が必要です。どうしても Leiningen やなんだ、って方が居たら Jar 版を配布するかもしれません。 Issue で急かして下さい。
  #+begin_src shell
lein preprocess-corpus -r /home/meguru/Documents/nico-dict/zips
#+end_src


** 前処理で行なっていること
   前処理で行なっていることは大きく分けて3つあります。
   1. HTML -> JSON 化    
      解析を容易にするために HTML を JSON にパースしています。タグとアトリビュート、コンテンツを分離することが出来るため、必要な情報のアクセスが容易になります。
   2. HTMLの一部のタグの削除     
      同様の理由でHTML タグの一部を削除しています。削除するタグは [[./remove-tags.org]] にあります。
